# Text-Summerizer-v2.0
## #OVERVIEW ğŸ“•
â˜ Summary generation is a natural language processing task that involves creating a concise and coherent summary of a longer piece of text. There are various ways to approach this task, ranging from simple rule-based methods to more complex machine learning models. In this blog post, we will look at three different approaches for generating summaries: using a pre-trained language model, using a TfidfVectorizer, and using nltk's cosine distance.

â˜ The first approach involves using a TfidfVectorizer to generate a summary. This approach involves creating a vector representation of each sentence in the text and then ranking the sentences based on their relevance. To implement this approach, we first need to import the required libraries and create a summarizer function that takes in a piece of text and returns the summary. The function begins by tokenizing the text using spacy and then creating a TfidfVectorizer object. The TfidfVectorizer is then fit on the list of sentences and used to transform the sentences into vectors. The vectors are then summed and the top N sentences are selected as the summary, where N is the number of sentences we want in the summary.

â˜ The second approach involves using nltk's cosine distance to generate a summary. This approach involves measuring the similarity between sentences and selecting the most similar sentences as the summary. To implement this approach, we first need to import the required libraries and create a generate_summary function that takes in a file name and returns the summary. The function begins by reading in the file and tokenizing the sentences using nltk's stopwords. Next, a similarity matrix is created by comparing each sentence to every other sentence in the text. The matrix is then converted into a graph using networkx and the PageRank algorithm is used to rank the sentences based on their importance. The top N sentences are then selected as the summary.

â˜ The third approach involves using a pre-trained language model, such as T5, to generate a summary. The T5 model was trained on a large dataset of text and is able to generate human-like summaries by predicting the next word in a sequence. To use this approach, we first need to install the required libraries and load the data. We can then create a NewsSummaryDataset and a NewsSummaryDataModule to handle the data processing and loading. Next, we can define our model by creating a T5ForConditionalGeneration model and specifying the hyperparameters. Finally, we can train the model by defining an AdamW optimizer and a ModelCheckpoint callback to save the model weights at each epoch.


## #MODELS ğŸ†
â˜[`HEADLINE`](https://drive.google.com/file/d/1o6gA5ofRYUL0jCUtjrNcnt2SJN_tAi2n/view?usp=sharing) MODEL

â˜[`SUMMARY 3`](https://drive.google.com/file/d/1P87tKfRhVCUtMRDJwh21zs1SSRcTwLY9/view?usp=sharing) MODEL


## #BLUEPRINT ğŸ›°ï¸
ğŸ›‘ All files must be arrange in given manner (including MODELS) ğŸ›‘

``` bash
Text-Summarizer-v2.0
|
â”‚   content.txt
â”‚   headline.py
â”‚   main.py
â”‚   pdf_txt.py
â”‚   README.md
â”‚   requirements.txt
â”‚   summarizer1.py
â”‚   summarizer2.py
â”‚   summarizer3.py   
â”‚   
â”œâ”€â”€â”€models
â”‚   â”œâ”€â”€â”€headline_model
â”‚   â”‚       config.json
â”‚   â”‚       pytorch_model.bin
â”‚   â”‚       special_tokens_map.json
â”‚   â”‚       spiece.model
â”‚   â”‚       tokenizer.json
â”‚   â”‚       tokenizer_config.json
â”‚   â”‚       
â”‚   â””â”€â”€â”€summary_model
â”‚          best-checkpoint.ckpt
â”‚          model.py
â”‚                      
â”œâ”€â”€â”€screenshots
â”‚       Exception.png
â”‚       File_upload.png
â”‚       Home.png
â”‚       summary_of _uploaded file.png
â”‚       Summery_of_inputText.png
â”‚       Text_input.png
â”‚       
â””â”€â”€â”€uploaded_pdfs
       Independence.pdf
```

## #EXPLAINED IN DETAIL...ğŸŒ


## #OUTPUTS ğŸš€
â˜ IN [`screenshots`](https://github.com/shrey-patel-3287/Text-Summerizer-v2.0/tree/main/screenshots) FOLDER


## #ISSUES ğŸ’€
â˜ Headline only work properly if given input is news article.

â˜ We are working on it...........
